# Import necessary libraries
import torch
import ollama
import base64
import cv2
import os
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.sqlite import SqliteSaver 
from typing import TypedDict


# Determine the best available device for inference
# Priority: CUDA (NVIDIA GPU) > MPS (Apple Silicon) > CPU
def get_device():
    """
    Detect and return the best available compute device.
    Returns 'cuda' for NVIDIA GPUs, 'mps' for Apple Silicon, or 'cpu' as fallback.
    """
    if torch.cuda.is_available():
        print("Using CUDA (NVIDIA GPU) for inference")
        return "cuda"
    elif torch.backends.mps.is_available():
        print("Using MPS (Apple Silicon) for inference")
        return "mps"
    else:
        print("Using CPU for inference")
        return "cpu"
    

# =============================================================================
# STATE DEFINITION
# =============================================================================
# The state is a TypedDict that flows through all nodes in the graph.
# Each node can read from and write to specific fields in the state.
# LangGraph automatically merges the returned dict from each node into the state.

class AgentState(TypedDict):
    """
    State object that flows through the LangGraph nodes.

    Fields:
    - user_input: The text entered by the user (set by get_user_input node)
    - should_exit: Boolean flag indicating if user wants to quit (set by get_user_input node)
    - llm_response: The response generated by the LLM (set by call_llm node)

    State Flow:
    1. Initial state: all fields empty/default
    2. After get_user_input: user_input and should_exit are populated
    3. After call_llm: llm_response is populated
    4. After print_response: state unchanged (node only reads, doesn't write)

    The graph loops continuously:
        get_user_input -> [conditional] -> call_llm -> print_response -> get_user_input
                              |
                              +-> END (if user wants to quit)
    """
    user_input: str
    should_exit: bool
    model_response: str
    chat_history: list  
    images: list

def create_graph():
    """
    Create the LangGraph state graph with three separate nodes:
    1. get_user_input: Reads input from stdin
    2. call_llm: Sends input to the LLM and gets response
    3. print_response: Prints the LLM's response to stdout

    Graph structure with conditional routing and internal loop:
        START -> get_user_input -> [conditional] -> call_llm -> print_response -+
                       ^                 |                                       |
                       |                 +-> END (if user wants to quit)         |
                       |                                                         |
                       +---------------------------------------------------------+

    The graph runs continuously until the user types 'quit', 'exit', or 'q'.
    """

    # =========================================================================
    # NODE 1: get_user_input
    # =========================================================================
    # This node reads a line of text from stdin and updates the state.
    # State changes:
    #   - user_input: Set to the text entered by the user
    #   - should_exit: Set to True if user typed quit/exit/q, False otherwise
    #   - llm_response: Unchanged (not modified by this node)
    def get_user_input(state: AgentState) -> dict:
        """
        Node that prompts the user for input via stdin.

        Reads state: Nothing (fresh input each iteration)
        Updates state:
            - user_input: The raw text entered by the user
            - should_exit: True if user wants to quit, False otherwise
        """
        # Display banner before each prompt
        print("\n" + "=" * 50)
        print("Enter your text (or 'quit' to exit):")
        print("=" * 50)

        print("\n> ", end="")
        user_input = input()

        with open("photo.jpg", "rb") as f:
            img_b64 = base64.b64encode(f.read()).decode("utf-8")

        # Check if user wants to exit
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("Goodbye!")
            return {
                "user_input": user_input,
                "should_exit": True        # Signal to exit the graph
            }
        

        system_prompt = [{"role": "system", "content": "You are a helpful assistant."}] #TASK 5: initialize a system prompt to begin the chat history
        chat_history = state.get("chat_history", system_prompt) #TASK 5: get the current state or initialize it with a system prompt
        
        return { #Otherwise, accept the user input and move forward
            "user_input" : user_input,
            "should_exit" : False,
            "chat_history" : chat_history, 
            "images" : img_b64,
        }
    

    def run_model(state: AgentState) -> dict: 
        """
        Node that invokes the LLM with the user's input.

        Reads state:
            - user_input: The text to send to the LLM
        Updates state:
            - llm_response: The text generated by the LLM
        """

        user_input = state["user_input"]

        images = state["images"]

        # Format the prompt for the instruction-tuned model
        prompt = f"User: {user_input}\nAssistant:"

        # Invoke the LLM and get the response
        chat_history = state["chat_history"] + [{"role": "user", "content": user_input}] #TASK 5: add the user input to the chat history
        
        response = ollama.chat(
            model='llava',
            messages=[{
                'role': 'user',
                'content': prompt,
                'images': images
            }]
        )

        response = response['message']['content']

        chat_history.append({"role": "assistant", "content": response}) 

        # Return only the field we're updating
        return {"model_response": response, 
                "chat_history": chat_history,
                } 
    

    # =========================================================================
    # NODE 3: print_response
    # =========================================================================
    # This node reads the LLM response from state and prints it to stdout.
    # State changes:
    #   - No changes (this node only reads state, doesn't modify it)
    def print_response(state: AgentState) -> dict:
        """
        Node that prints the LLM's response to stdout.

        Reads state:
            - llm_response: The text to print
        Updates state:
            - Nothing (returns empty dict, state unchanged)
        """

        chat_history = state["chat_history"]

        print("\n" + "-" * 50)
        print("Model Response:")
        print("-" * 50)
        print(state["model_response"])
    
        # Return empty dict - no state updates from this node
        return {}
    

    # =========================================================================
    # ROUTING FUNCTION
    # =========================================================================
    # This function examines the state and determines which node to go to next.
    # It's used for conditional edges after get_user_input.
    # Two possible routes:
    #   1. User wants to quit -> END
    #   2. User entered any input -> proceed to call_llm
    def route_after_input(state: AgentState) -> str:
        """
        Routing function that determines the next node based on state.

        Examines state:
            - should_exit: If True, terminate the graph

        Returns:
            - "__end__": If user wants to quit
            - "call_llm": If user provided any input (including empty)
        """
        # Check if user wants to exit
        if state.get("should_exit", False):
            return END

        user_input = state["user_input"]

        if user_input == "": 
            return "get_user_input"

        return "run_model"
    

    # =========================================================================
    # GRAPH CONSTRUCTION
    # =========================================================================
    # Create a StateGraph with our defined state structure
    graph_builder = StateGraph(AgentState)

    # Add all three nodes to the graph
    graph_builder.add_node("get_user_input", get_user_input)
    graph_builder.add_node("run_model", run_model)
    graph_builder.add_node("print_response", print_response)

    # Define edges:
    # 1. START -> get_user_input (always start by getting user input)
    graph_builder.add_edge(START, "get_user_input")

    # 2. get_user_input -> [conditional] -> call_llm OR END
    #    Uses route_after_input to decide based on state.should_exit
    graph_builder.add_conditional_edges(
        "get_user_input",      # Source node
        route_after_input,      # Routing function that examines state
        {
            "run_model" : "run_model", 
            END: END                  # Quit command -> terminate graph
        }
    )

    graph_builder.add_edge("run_model", "print_response")
    graph_builder.add_edge("print_response", "get_user_input")

    return graph_builder


def save_graph_image(graph, filename="lg_graph.png"):
    """
    Generate a Mermaid diagram of the graph and save it as a PNG image.
    Uses the graph's built-in Mermaid export functionality.
    """
    try:
        # Get the Mermaid PNG representation of the graph
        # This requires the 'grandalf' package for rendering
        png_data = graph.get_graph(xray=True).draw_mermaid_png()

        # Write the PNG data to file
        with open(filename, "wb") as f:
            f.write(png_data)

        print(f"Graph image saved to {filename}")
    except Exception as e:
        print(f"Could not save graph image: {e}")
        print("You may need to install additional dependencies: pip install grandalf")


def main():

    print("=" * 50)
    print("LangGraph Agent with LLaVA")
    print("=" * 50)
    print()


    # Step 2: Build the LangGraph with the LLM
    print("\nCreating LangGraph...")
    graph_builder = create_graph()
    print("Graph created successfully!")

    with SqliteSaver.from_conn_string("checkpoints.db") as checkpointer:

        graph = graph_builder.compile(checkpointer=checkpointer)

        thread_id = "workflow_1"
        config = {"configurable": {"thread_id": thread_id}}

        # Check for existing state
        current_state = graph.get_state(config)


        print("\nSaving graph visualization...")
        save_graph_image(graph)

  
        initial_state: AgentState = {
            "user_input": "",
            "should_exit": False,
            "model_response": "",
        }

        # Single invocation - the graph loops internally via print_response -> get_user_input
        # The graph only exits when route_after_input returns END (user typed quit/exit/q)
        #graph.invoke(initial_state)

        if current_state.next:
            print("ğŸ”„ Found incomplete workflow. Resuming...")
            print(f"   Completed so far: {current_state.values.get('messages', [])}")
            print(f"   Resuming at: {current_state.next}")
            graph.invoke(None, config=config)
        else:
            print("â–¶ï¸  Starting new workflow...")
            graph.invoke(initial_state, config=config)

# Entry point - only run main() if this script is executed directly
if __name__ == "__main__":
    main()











if __name__ == "__main__":
    pass