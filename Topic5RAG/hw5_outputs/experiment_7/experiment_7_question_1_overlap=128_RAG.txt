Overlap: 128

Question: What are the risks of AI systems built to detect human emotions?

Context: [Source: EU_AI_Act.txt, Relevance: 0.726]
ted because that practice adds to the feeling of mass surveillance and can lead to gross
violations of fundamental rights, including the right to privacy.

(44)

There are serious concerns about the scientific basis of AI systems aiming to identify or infer emotions, particularly
as expression of emotions vary considerably across cultures and situations, and even within a single individual.
Among the key shortcomings of such systems are the limited reliability, the lack of specificity and the limited
genera

---

[Source: EU_AI_Act.pdf, Relevance: 0.644]
systems, such systems could lead to detrimental or unfavourable treatment of certain 
natural persons or whole groups thereof. Therefore, the placing on the market, the putting into service, or the use of 
AI systems intended to be used to detect the emotional state of individuals in situations related to the workplace and 
education should be prohibited.

---

[Source: EU_AI_Act.txt, Relevance: 0.643]
based on the inference of those attributes or characteristics;
(c) AI systems intended to be used for emotion recognition.

2.

Critical infrastructure: AI systems intended to be used as safety components in the management and operation of
critical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity.

3.

---

[Source: EU_AI_Act.pdf, Relevance: 0.643]
ng the key shortcomings of such systems are the limited reliability, the lack of specificity and the limited 
generalisability. Therefore, AI systems identifying or inferring emotions or intentions of natural persons on the basis 
of their biometric data may lead to discriminatory outcomes and can be intrusive to the rights and freedoms of the 
concerned persons.

---

[Source: EU_AI_Act.txt, Relevance: 0.638]
dividual.
Among the key shortcomings of such systems are the limited reliability, the lack of specificity and the limited
generalisability. Therefore, AI systems identifying or inferring emotions or intentions of natural persons on the basis
of their biometric data may lead to discriminatory outcomes and can be intrusive to the rights and freedoms of the
concerned persons.

The risks associated with AI systems designed to detect human emotions include:

1. **Mass Surveillance Concerns**: These systems might increase feelings of being monitored, potentially leading to violations of fundamental rights like privacy.
   
2. **Cultural Sensitivity Issues**: Emotions vary significantly across different cultures and contexts, making it challenging for these systems to accurately interpret expressions reliably.

3. **Reliability and Specificity Shortcomings**: Such systems often struggle with consistency (reliability), detailed accuracy, and broad applicability (specificity).

4. **Potential Discrimination**: There's a risk that these systems could unfairly target specific individuals or groups due to their limitations in detecting emotions effectively.

5. **Intrusiveness to Rights and Freedoms**: Using biometric data to infer emotions could infringe upon personal rights and freedoms without proper safeguards. 

These points highlight the complex challenges and potential drawbacks when developing AI tools aimed at understanding human emotions.