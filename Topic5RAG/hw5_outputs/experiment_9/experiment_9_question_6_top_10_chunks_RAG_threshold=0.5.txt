Question: Risks of AI emotion detectors

Context: [Source: EU_AI_Act.txt, Relevance: 0.659]
ted because that practice adds to the feeling of mass surveillance and can lead to gross
violations of fundamental rights, including the right to privacy.

(44)

There are serious concerns about the scientific basis of AI systems aiming to identify or infer emotions, particularly
as expression of emotions vary considerably across cultures and situations, and even within a single individual.
Among the key shortcomings of such systems are the limited reliability, the lack of specificity and the limited
genera

---

[Source: EU_AI_Act.pdf, Relevance: 0.598]
hysical, psychological health or financial interests are likely to occur, are 
particularly dangerous and should therefore be prohibited. Such AI systems deploy subliminal components such as 
audio, image, video stimuli that persons cannot perceive, as those stimuli are beyond human perception, or other 
manipulative or deceptive techniques that subvert or impair person’s autonomy, decision-making or free choice in 
ways that people are not consciously aware of those techniques or, where they are aware of t

---

[Source: EU_AI_Act.txt, Relevance: 0.596]
sical, psychological health or financial interests are likely to occur, are
particularly dangerous and should therefore be prohibited. Such AI systems deploy subliminal components such as
audio, image, video stimuli that persons cannot perceive, as those stimuli are beyond human perception, or other
manipulative or deceptive techniques that subvert or impair person’s autonomy, decision-making or free choice in
ways that people are not consciously aware of those techniques or, where they are aware of them, c

---

[Source: EU_AI_Act.pdf, Relevance: 0.596]
xpand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV 
footage, should be prohibited because that practice adds to the feeling of mass surveillance and can lead to gross 
violations of fundamental rights, including the right to privacy.
(44)
There are serious concerns about the scientific basis of AI systems aiming to identify or infer emotions, particularly 
as expression of emotions vary considerably across cultures and situations, and even within a

---

[Source: EU_AI_Act.txt, Relevance: 0.584]
based on the inference of those attributes or characteristics;
(c) AI systems intended to be used for emotion recognition.

2.

Critical infrastructure: AI systems intended to be used as safety components in the management and operation of
critical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity.

3.

---

[Source: EU_AI_Act.pdf, Relevance: 0.578]
rast, the biometric data 
has already been captured and the comparison and identification occur only after a significant delay. This involves 
material, such as pictures or video footage generated by closed circuit television cameras or private devices, which 
has been generated before the use of the system in respect of the natural persons concerned.
(18)
The notion of ‘emotion recognition system’ referred to in this Regulation should be defined as an AI system for the 
purpose of identifying or inferring

---

[Source: EU_AI_Act.txt, Relevance: 0.574]
s or private devices, which
has been generated before the use of the system in respect of the natural persons concerned.

(18)

The notion of ‘emotion recognition system’ referred to in this Regulation should be defined as an AI system for the
purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data.
The notion refers to emotions or intentions such as happiness, sadness, anger, surprise, disgust, embarrassment,
excitement, shame, contempt, satisfacti

---

[Source: EU_AI_Act.txt, Relevance: 0.557]
er this Regulation, and emotion recognition systems that are not prohibited under this Regulation,
should be classified as high-risk. Biometric systems which are intended to be used solely for the purpose of enabling
cybersecurity and personal data protection measures should not be considered to be high-risk AI systems.

(55)

---

[Source: EU_AI_Act.txt, Relevance: 0.546]
of a natural person
for the sole purpose of having access to a service, unlocking a device or having secure access to premises. In addition,
AI systems intended to be used for biometric categorisation according to sensitive attributes or characteristics
protected under Article 9(1) of Regulation (EU) 2016/679 on the basis of biometric data, in so far as these are not
prohibited under this Regulation, and emotion recognition systems that are not prohibited under this Regulation,
should be classified as high-

---

[Source: EU_AI_Act.pdf, Relevance: 0.521]
age or disability should be taken 
into account to the extent the AI system is intended to interact with those groups as well. Moreover, natural persons 
should be notified when they are exposed to AI systems that, by processing their biometric data, can identify or infer 
the emotions or intentions of those persons or assign them to specific categories.

The risks associated with AI emotion detectors include:

1. **Mass Surveillance Concerns**: There are serious concerns regarding the potential for mass surveillance practices, which could lead to violations of fundamental rights like the right to privacy.

2. **Scientific Basis Issues**: There are also issues related to the scientific foundation of AI systems designed to detect or infer emotions. Expressions of emotions can vary significantly across different cultures and situations, making it difficult for such systems to reliably and accurately identify emotions.

3. **Ethical Considerations**: The ethical implications of using AI to detect emotions raise concerns about the manipulation or deception of individuals' autonomy and decision-making processes without their awareness.

4. **Privacy Risks**: The context mentions that the collection and analysis of biometric data may involve capturing data from various sources, including social media and CCTV footage, potentially leading to unauthorized exposure of personal information.

These points highlight the multifaceted nature of the risks involved in developing and deploying AI emotion detection technologies.