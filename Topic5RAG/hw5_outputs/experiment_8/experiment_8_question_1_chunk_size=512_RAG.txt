Chunk Size: 512

Question: What are the risks of AI systems built to detect human emotions?

Context: [Source: EU_AI_Act.txt, Relevance: 0.726]
ted because that practice adds to the feeling of mass surveillance and can lead to gross
violations of fundamental rights, including the right to privacy.

(44)

There are serious concerns about the scientific basis of AI systems aiming to identify or infer emotions, particularly
as expression of emotions vary considerably across cultures and situations, and even within a single individual.
Among the key shortcomings of such systems are the limited reliability, the lack of specificity and the limited
genera

---

[Source: EU_AI_Act.pdf, Relevance: 0.644]
systems, such systems could lead to detrimental or unfavourable treatment of certain 
natural persons or whole groups thereof. Therefore, the placing on the market, the putting into service, or the use of 
AI systems intended to be used to detect the emotional state of individuals in situations related to the workplace and 
education should be prohibited.

---

[Source: EU_AI_Act.txt, Relevance: 0.643]
based on the inference of those attributes or characteristics;
(c) AI systems intended to be used for emotion recognition.

2.

Critical infrastructure: AI systems intended to be used as safety components in the management and operation of
critical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity.

3.

---

[Source: EU_AI_Act.pdf, Relevance: 0.643]
ng the key shortcomings of such systems are the limited reliability, the lack of specificity and the limited 
generalisability. Therefore, AI systems identifying or inferring emotions or intentions of natural persons on the basis 
of their biometric data may lead to discriminatory outcomes and can be intrusive to the rights and freedoms of the 
concerned persons.

---

[Source: EU_AI_Act.txt, Relevance: 0.638]
dividual.
Among the key shortcomings of such systems are the limited reliability, the lack of specificity and the limited
generalisability. Therefore, AI systems identifying or inferring emotions or intentions of natural persons on the basis
of their biometric data may lead to discriminatory outcomes and can be intrusive to the rights and freedoms of the
concerned persons.

The risks associated with AI systems designed to detect human emotions include:

1. **Mass Surveillance Concerns**: These systems can add to feelings of mass surveillance, which is concerning due to potential violations of fundamental rights like privacy.

2. **Cultural Variability**: Emotions expressed by individuals can vary significantly across different cultures and situations. This variability poses challenges for creating universally reliable and specific AI systems.

3. **Reliability Issues**: Limited reliability means these systems might not consistently accurately predict emotions, leading to inaccuracies in their outputs.

4. **Specificity Problems**: Lack of specificity suggests that these systems may struggle to differentiate between various types of emotions, making them less effective at detecting nuanced emotional states.

5. **Generalisability Limitations**: The systems' inability to generalize well across diverse contexts indicates they may perform poorly outside the environments where they were trained.

These factors collectively highlight significant risks in deploying AI systems aimed at detecting human emotions, especially given the complexities involved in understanding and predicting human behavior through technology.