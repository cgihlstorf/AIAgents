## Experiment 1

### Does the model hallucinate specific values without RAG?

The model did hallucinate some specific values. For example, when asked what type of oil should be used in a Model T, the model responded 
with "motor oil" as the answer, but a quick command+f search through the manual did not find any instances of that phrase. In another instance 
when asked what Mr. Flood said about Mayor David Black, the model stated that Mayor Black was the mayor of New York, when in reality 
he was the mayor of Papillion, Nebraska. The model also completely hallucinated the answer to the question 
"What mistake Elise Stefanovic make in Congress on January 23, 2026?", claiming that her mistake was saying that the United States was not a
democracy when in reality the mistake was that she accidentally cast a "no" vote instead of a "yes" vote.

### Does RAG ground the answers in the actual manual?




### Are there questions where the model's general knowledge is actually correct?
